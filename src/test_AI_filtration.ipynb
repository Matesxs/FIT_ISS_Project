{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, optimizers, Model, callbacks, initializers, utils\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from pydub import AudioSegment\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "\n",
    "DATA_PATH = r\"..\\\\data\"\n",
    "TRAIN_DATASET_PATH = r\"..\\\\train_dataset\"\n",
    "TEST_DATASET_PATH = r\"..\\\\test_dataset\"\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "MAX_SAMPLE_FREQUENCY = 256_000\n",
    "\n",
    "FRAME_SIZE = 4096\n",
    "FRAME_OVERLAP = 2048\n",
    "\n",
    "MAXIMUM_NUMBER_OF_NOISE_CHANNELS = 8\n",
    "NOISE_APLITUDE_MAX = 4\n",
    "NOISE_FREQ_MAX = 48_000\n",
    "\n",
    "PRETRAIN_EPOCHS = 2\n",
    "EPOCHS = 40\n",
    "STEPES_PER_EPOCH = 5_000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "frame_shift = FRAME_SIZE - FRAME_OVERLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(files, dirs=[], extensions=[]):\n",
    "    new_dirs = []\n",
    "    for d in dirs:\n",
    "        if not os.path.exists(d): continue\n",
    "        \n",
    "        try:\n",
    "            new_dirs += [ os.path.join(d, f) for f in os.listdir(d) ]\n",
    "        except OSError:\n",
    "            if os.path.splitext(d)[1] in extensions:\n",
    "                files.append(d)\n",
    "\n",
    "    if new_dirs:\n",
    "        find_files(files, new_dirs, extensions)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all mp3 files to wav\n",
    "def convert_all_files():\n",
    "    for file_path in file_paths:\n",
    "        cleaned_name = file_path[:-4]\n",
    "\n",
    "        if file_path[-4:] == \".mp3\":\n",
    "            new_name = cleaned_name + \".wav\"\n",
    "\n",
    "            sound = AudioSegment.from_file(file_path, format=\"mp3\")\n",
    "            sound.export(new_name, \"wav\")\n",
    "\n",
    "            os.remove(file_path)\n",
    "        elif file_path[-4:] == \".mp4\":\n",
    "            subprocess.Popen(f\"ffmpeg -i {file_path} -vn -acodec pcm_s16le -ar 44100 -ac 2 {cleaned_name}.wav\").wait()\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(samples:np.ndarray):\n",
    "    smpl = samples.copy()\n",
    "    max_abs_val = max(abs(smpl))\n",
    "\n",
    "    # Normalization\n",
    "    smpl /= max_abs_val\n",
    "    return smpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_data(data, fs):\n",
    "  sdata = data.copy()\n",
    "\n",
    "  length_in_secs = sdata.size / fs\n",
    "  time = np.linspace(0, length_in_secs, sdata.size, endpoint=False)\n",
    "\n",
    "  for _ in range(random.randint(1, MAXIMUM_NUMBER_OF_NOISE_CHANNELS)):\n",
    "    sdata += (random.random() * NOISE_APLITUDE_MAX) * np.cos(2 * np.pi * (random.random() * NOISE_FREQ_MAX) * time + (random.random() * 2 * np.pi))\n",
    "  \n",
    "  return sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frames(data:np.ndarray):\n",
    "  number_of_frames = data.size // frame_shift\n",
    "  frames = [data[idx * frame_shift : FRAME_SIZE + idx * frame_shift] for idx in range(number_of_frames)]\n",
    "  frames[len(frames) - 1] = np.pad(frames[len(frames) - 1], (0, FRAME_SIZE - frames[len(frames) - 1].shape[0]), \"constant\")\n",
    "  return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawFFT(signal, frequency, idx=None):\n",
    "  fft = np.fft.fft(signal)\n",
    "\n",
    "  data_length = len(signal)\n",
    "  freq = np.arange(data_length)/(data_length/frequency)\n",
    "\n",
    "  half_length = data_length//2\n",
    "\n",
    "  freq_one_side = freq[:half_length]\n",
    "  fft = fft[:half_length]/half_length\n",
    "\n",
    "  plt.figure(figsize=(18,8))\n",
    "  plt.title(f\"FFT {idx}\" if idx is not None else \"Generic FFT\")\n",
    "  plt.plot(freq_one_side, abs(fft))\n",
    "  plt.xlabel('f[Hz]')\n",
    "  plt.ylabel('Amplituda[-]')\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_file(smpls, fs):\n",
    "  # Split and normalize\n",
    "  try:\n",
    "    if smpls.shape[1] > 0:\n",
    "      normalized_samples = []\n",
    "\n",
    "      for i in range(smpls.shape[1]):\n",
    "        normalized_samples.extend(normalization(smpls[:, i]))\n",
    "\n",
    "      normalized_samples = np.array(normalized_samples)\n",
    "    else:\n",
    "      normalized_samples = normalization(smpls)\n",
    "  except Exception as e:\n",
    "    print(f\"Exception on spliting data\\n{e}\")\n",
    "    normalized_samples = normalization(smpls)\n",
    "\n",
    "  # Create noised data and normalize them\n",
    "  noised_samples = noise_data(normalized_samples, fs)\n",
    "  noised_samples = normalization(noised_samples)\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(normalized_samples.size) / fs, normalized_samples)\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(noised_samples.size) / fs, noised_samples)\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  # Power density\n",
    "  # f1, Pxx1 = signal.periodogram(normalized_samples, fs, scaling=\"density\")\n",
    "  # print(f1.shape)\n",
    "  # print(Pxx1.shape)\n",
    "\n",
    "  # plt.semilogy(f1, Pxx1)\n",
    "  # plt.title(\"original\")\n",
    "  # plt.xlabel('f[Hz]')\n",
    "  # plt.ylabel('PSD [V**2/Hz]')\n",
    "  # plt.show()\n",
    "\n",
    "  # f2, Pxx2 = signal.periodogram(noised_samples, fs, scaling=\"density\")\n",
    "  # print(f2.shape)\n",
    "  # print(Pxx2.shape)\n",
    "\n",
    "  # plt.semilogy(f2, Pxx2)\n",
    "  # plt.title(\"noised\")\n",
    "  # plt.xlabel('f[Hz]')\n",
    "  # plt.ylabel('PSD [V**2/Hz]')\n",
    "  # plt.show()\n",
    "\n",
    "  # FFT\n",
    "  # normal_fft = np.fft.fft(normalized_samples)\n",
    "  # noise_fft = np.fft.fft(noised_samples)\n",
    "\n",
    "  # print(normal_fft.shape)\n",
    "  # print(noise_fft.shape)\n",
    "\n",
    "  # data_length = len(normal_fft)\n",
    "  # freq = np.arange(data_length)/(data_length/fs)\n",
    "\n",
    "  # half_length = data_length//2\n",
    "\n",
    "  # freq_one_side = freq[:half_length]\n",
    "  # normal_fft = normal_fft[:half_length]/half_length\n",
    "  # noise_fft = noise_fft[:half_length]/half_length\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.title(f\"Normal\")\n",
    "  # plt.plot(freq_one_side, abs(normal_fft))\n",
    "  # plt.xlabel('f[Hz]')\n",
    "  # plt.ylabel('Amplituda[-]')\n",
    "  # plt.tight_layout()\n",
    "  # plt.show()\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.title(f\"Noise\")\n",
    "  # plt.plot(freq_one_side, abs(noise_fft))\n",
    "  # plt.xlabel('f[Hz]')\n",
    "  # plt.ylabel('Amplituda[-]')\n",
    "  # plt.tight_layout()\n",
    "  # plt.show()\n",
    "\n",
    "  # Set signals to be between 0 and 1\n",
    "  normalized_samples += 1\n",
    "  noised_samples += 1\n",
    "  normalized_samples = normalization(normalized_samples)\n",
    "  noised_samples = normalization(noised_samples)\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(normalized_samples.size) / fs, normalized_samples)\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(noised_samples.size) / fs, noised_samples)\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  # Create frames\n",
    "  normalized_samples = create_frames(normalized_samples)\n",
    "  noised_samples = create_frames(noised_samples)\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(normalized_samples[50].size) / fs, normalized_samples[50])\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(noised_samples[50].size) / fs, noised_samples[50])\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  return normalized_samples, noised_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to convert\n",
      "0\n",
      "Files to sort\n",
      "73745\n",
      "Moving files\n"
     ]
    }
   ],
   "source": [
    "def move_file(src, dest):\n",
    "  name = Path(src).name\n",
    "  shutil.move(src, os.path.join(dest, name))\n",
    "\n",
    "def move_files(file_paths, target_path):\n",
    "  for file_path in file_paths:\n",
    "    move_file(file_path, target_path)\n",
    "\n",
    "file_paths = []\n",
    "find_files(file_paths, dirs=[DATA_PATH], extensions=[\".mp3\", \".mp4\"])\n",
    "print(\"Files to convert\")\n",
    "print(len(file_paths))\n",
    "\n",
    "convert_all_files()\n",
    "    \n",
    "already_used_filenames = []\n",
    "find_files(already_used_filenames, dirs=[TEST_DATASET_PATH, TRAIN_DATASET_PATH], extensions=[\".npy\"])\n",
    "\n",
    "if not os.path.exists(\"tmp_dataset\"):\n",
    "  os.mkdir(\"tmp_dataset\")\n",
    "\n",
    "  file_paths = []\n",
    "  find_files(file_paths, dirs=[DATA_PATH], extensions=[\".wav\"])\n",
    "\n",
    "  print(\"Numbe of source files\")\n",
    "  print(len(file_paths))\n",
    "\n",
    "  for file_path in tqdm(file_paths):\n",
    "    cleaned_name = Path(file_path).name[:-4]\n",
    "\n",
    "    if (any([cleaned_name in file_name for file_name in already_used_filenames]) or\n",
    "        any([cleaned_name in file_name for file_name in os.listdir(\"tmp_dataset\")])):\n",
    "      print(f\"Skipping {cleaned_name}\")\n",
    "      continue\n",
    "\n",
    "    smpls, f = sf.read(file_path)\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Processing {cleaned_name}\")\n",
    "        \n",
    "    random.seed()\n",
    "    norm_s, nois_s = process_one_file(smpls, f)\n",
    "\n",
    "    for idx, (a, b) in enumerate(zip(norm_s, nois_s)):\n",
    "      if not os.path.exists(f\"tmp_dataset/{idx}_{cleaned_name}_{f}\"):\n",
    "        np.save(f\"tmp_dataset/{idx}_{cleaned_name}_{f}\", np.array([a, b, np.fft.fft(b)]))\n",
    "    gc.collect()\n",
    "\n",
    "file_paths = []\n",
    "find_files(file_paths, dirs=[\"tmp_dataset\"], extensions=[\".npy\"])\n",
    "number_of_files = len(file_paths)\n",
    "\n",
    "print(\"Files to sort\")\n",
    "print(number_of_files)\n",
    "\n",
    "if number_of_files > 0:\n",
    "  random.shuffle(file_paths)\n",
    "\n",
    "  if not os.path.exists(TEST_DATASET_PATH):\n",
    "    os.mkdir(TEST_DATASET_PATH)\n",
    "\n",
    "  if not os.path.exists(TRAIN_DATASET_PATH):\n",
    "    os.mkdir(TRAIN_DATASET_PATH)\n",
    "\n",
    "  print(\"Moving files\")\n",
    "  valid_file_count = int(number_of_files * VAL_SPLIT)\n",
    "  move_files(file_paths[:valid_file_count], TEST_DATASET_PATH)\n",
    "  move_files(file_paths[valid_file_count:], TRAIN_DATASET_PATH)\n",
    "\n",
    "shutil.rmtree(\"tmp_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_imag_to_parts(array:np.ndarray):\n",
    "    tmp = np.empty((array.shape[0], 2))\n",
    "\n",
    "    for idx, a in enumerate(array):\n",
    "        tmp[idx][0] = a.real\n",
    "        tmp[idx][1] = a.imag\n",
    "\n",
    "    return np.nan_to_num(tmp)\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence, threading.Thread):\n",
    "    def __init__(self, path, dim, batch_size=32, shuffle=True):\n",
    "        super(DataGenerator, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.files = []\n",
    "        find_files(self.files, dirs=[path], extensions=[\".npy\"])\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.queue = []\n",
    "\n",
    "        self.length = int(np.floor(len(self.files) / self.batch_size))\n",
    "        self.index = 0\n",
    "        self.daemon = True\n",
    "        self.__terminate = False;\n",
    "        self.shuffle_data()\n",
    "        self.start()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.__terminate = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def run(self) -> None:\n",
    "        while True:\n",
    "            if self.__terminate: break\n",
    "\n",
    "            if len(self.queue) < 20:\n",
    "                if (self.index + 1) >= self.length:\n",
    "                    self.shuffle_data()\n",
    "                    self.index = 0\n",
    "\n",
    "                files = self.files[self.index*self.batch_size:(self.index+1)*self.batch_size]\n",
    "                self.index += 1\n",
    "\n",
    "                # Generate data\n",
    "                data = self.__data_generation(files)\n",
    "                if data is not None:\n",
    "                    self.queue.append(data)\n",
    "            else: time.sleep(0.1)\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        while len(self.queue) == 0: time.sleep(0.01)\n",
    "        return self.queue.pop()\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.files)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def __data_generation(self, files):\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        fft = np.empty((self.batch_size, self.dim[0], 2))\n",
    "        f = np.empty((self.batch_size), dtype=float)\n",
    "        y = np.empty((self.batch_size, *self.dim))\n",
    "\n",
    "        # Generate data\n",
    "        for idx, file in enumerate(files):\n",
    "            freq = float(file[:-4].split(\"_\")[-1])\n",
    "            loaded_data = np.load(file)\n",
    "\n",
    "            try:\n",
    "                X[idx,] = loaded_data[1].real\n",
    "                fft[idx,] = convert_imag_to_parts(loaded_data[2])\n",
    "                f[idx] = freq / MAX_SAMPLE_FREQUENCY\n",
    "                y[idx,] = loaded_data[0].real\n",
    "            except Exception:\n",
    "                return None\n",
    "\n",
    "        return [X, fft, f], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm_value = 2.0\n",
    "\n",
    "def create_model():\n",
    "  inp1 = layers.Input(shape=(FRAME_SIZE,1), name=\"frame_input\")\n",
    "  inp2 = layers.Input(shape=(FRAME_SIZE,2), name=\"fft_input\")\n",
    "  inp3 = layers.Input(shape=(1,), name=\"frequency_input\")\n",
    "\n",
    "  y = layers.Dense(FRAME_SIZE)(inp3)\n",
    "  y = layers.LeakyReLU(0.2)(y)\n",
    "  y = layers.Dropout(0.1)(y)\n",
    "  y = layers.Reshape((FRAME_SIZE, 1))(y)\n",
    "\n",
    "  x = layers.concatenate([inp1, inp2, y])\n",
    "\n",
    "  # Encoder\n",
    "  x = layers.Conv1D(256, kernel_size=3, strides=2, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n",
    "  x1 = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.Conv1D(128, kernel_size=3, strides=2, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x1)\n",
    "  x2 = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.Conv1D(128, kernel_size=3, strides=1, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.Conv1D(64, kernel_size=3, strides=2, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x2)\n",
    "  x3 = layers.LeakyReLU(0.2)(x)\n",
    "  \n",
    "  # Encoded value\n",
    "\n",
    "  # Edecoder\n",
    "  x = layers.Conv1D(64, kernel_size=3, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x3)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.Add()([x, x3])\n",
    "\n",
    "  x = layers.Conv1D(64, kernel_size=3, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n",
    "  x = layers.UpSampling1D(2)(x)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.Conv1D(128, kernel_size=3, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.Add()([x, x2])\n",
    "\n",
    "  x = layers.Conv1D(128, kernel_size=3, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n",
    "  x = layers.UpSampling1D(2)(x)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.Conv1D(256, kernel_size=3, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.Add()([x, x1])\n",
    "\n",
    "  x = layers.Conv1D(256, kernel_size=3, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n",
    "  x = layers.UpSampling1D(2)(x)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  # Output\n",
    "  x = layers.Conv1D(1, kernel_size=1, kernel_constraint=max_norm(max_norm_value), activation='sigmoid', padding='same')(x)\n",
    "  return Model([inp1, inp2, inp3], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33389\n",
      "8347\n"
     ]
    }
   ],
   "source": [
    "train_data_generator = DataGenerator(TRAIN_DATASET_PATH, (FRAME_SIZE,), BATCH_SIZE)\n",
    "test_data_generator = DataGenerator(TEST_DATASET_PATH, (FRAME_SIZE,), BATCH_SIZE)\n",
    "\n",
    "print(len(train_data_generator))\n",
    "print(len(test_data_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseToSignalLoss(y_true, y_pred):\n",
    "    losses = tf.math.divide(tf.math.reduce_sum(tf.math.pow(tf.math.abs(tf.math.subtract(y_true,y_pred)),2)),\n",
    "                            tf.math.reduce_sum(tf.math.pow(tf.math.abs(y_true),2)))\n",
    "    return tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNR(y_true, y_pred):\n",
    "  return -10.0 * K.log(K.mean(K.square(y_pred - y_true))) / K.log(10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " frequency_input (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096)         8192        ['frequency_input[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 4096)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096)         0           ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " frame_input (InputLayer)       [(None, 4096, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " fft_input (InputLayer)         [(None, 4096, 2)]    0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 4096, 1)      0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4096, 4)      0           ['frame_input[0][0]',            \n",
      "                                                                  'fft_input[0][0]',              \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 2048, 256)    3328        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 2048, 256)    0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 1024, 128)    98432       ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 1024, 128)    0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 512, 64)      24640       ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 512, 64)      0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 512, 64)      12352       ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 512, 64)      0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 512, 64)      0           ['leaky_re_lu_6[0][0]',          \n",
      "                                                                  'leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 512, 64)      12352       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 1024, 64)     0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 1024, 64)     0           ['up_sampling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 1024, 128)    24704       ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 1024, 128)    0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 1024, 128)    0           ['leaky_re_lu_8[0][0]',          \n",
      "                                                                  'leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 1024, 128)    49280       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 2048, 128)   0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 2048, 128)    0           ['up_sampling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 2048, 256)    98560       ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)     (None, 2048, 256)    0           ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 2048, 256)    0           ['leaky_re_lu_10[0][0]',         \n",
      "                                                                  'leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 2048, 256)    196864      ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " up_sampling1d_2 (UpSampling1D)  (None, 4096, 256)   0           ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)     (None, 4096, 256)    0           ['up_sampling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 4096, 1)      257         ['leaky_re_lu_11[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 528,961\n",
      "Trainable params: 528,961\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[(None, 4096, 1), (None, 4096, 2), (None, 1)]\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()\n",
    "utils.plot_model(model, show_shapes=True, expand_nested=True, dpi=312)\n",
    "print(model.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(1e-4, 0.5), loss=noiseToSignalLoss, metrics=[SNR])\n",
    "# model.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss=\"mse\", metrics=[SNR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint\n",
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\Desktop\\iss_project\\venv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/240\n",
      "5000/5000 [==============================] - ETA: 0s - loss: 0.0063 - SNR: 28.2510\n",
      "Epoch 00001: val_SNR improved from -inf to 29.65762, saving model to best_weights\n",
      "INFO:tensorflow:Assets written to: best_weights\\assets\n",
      "5000/5000 [==============================] - 1062s 205ms/step - loss: 0.0063 - SNR: 28.2510 - val_loss: 0.0044 - val_SNR: 29.6576 - lr: 1.0000e-04\n",
      "Epoch 2/240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\Desktop\\iss_project\\venv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "c:\\Users\\marti\\Desktop\\iss_project\\venv\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5000/5000 [==============================] - ETA: 0s - loss: 0.0047 - SNR: 29.5383\n",
      "Epoch 00002: val_SNR improved from 29.65762 to 30.34129, saving model to best_weights\n",
      "INFO:tensorflow:Assets written to: best_weights\\assets\n",
      "5000/5000 [==============================] - 1131s 226ms/step - loss: 0.0047 - SNR: 29.5383 - val_loss: 0.0038 - val_SNR: 30.3413 - lr: 1.0000e-04\n",
      "Epoch 3/240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\Desktop\\iss_project\\venv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "c:\\Users\\marti\\Desktop\\iss_project\\venv\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470/5000 [=============>................] - ETA: 7:17 - loss: 0.0042 - SNR: 29.9673"
     ]
    }
   ],
   "source": [
    "number_of_batches = len(train_data_generator) // STEPES_PER_EPOCH\n",
    "checkpoint = callbacks.ModelCheckpoint(\"best_weights\", monitor=\"val_SNR\", verbose=1, save_best_only=True, mode=\"max\")\n",
    "\n",
    "try:\n",
    "  if os.path.exists(\"checkpoint.h5\"):\n",
    "    print(\"Loading checkpoint\")\n",
    "    model.load_weights(\"checkpoint.h5\")\n",
    "  else:\n",
    "    print(\"Starting pretrain\")\n",
    "    model.fit(train_data_generator, steps_per_epoch=STEPES_PER_EPOCH, epochs=PRETRAIN_EPOCHS)\n",
    "    model.save_weights(\"checkpoint.h5\")\n",
    "\n",
    "  print(\"Starting training\")\n",
    "  model.fit(train_data_generator, steps_per_epoch=STEPES_PER_EPOCH, epochs=EPOCHS * number_of_batches,\n",
    "            callbacks=[callbacks.ReduceLROnPlateau(monitor='val_loss',patience=5,factor=0.7,verbose=1), callbacks.TensorBoard(\"logs\"), checkpoint],\n",
    "            validation_data=test_data_generator, validation_steps=(STEPES_PER_EPOCH // 4) if STEPES_PER_EPOCH is not None else None)\n",
    "except KeyboardInterrupt:\n",
    "  model.save_weights(\"checkpoint.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"best_weights\"): model.load_weights(\"best_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(sample, fs):\n",
    "  number_of_frames = sample.size // FRAME_SIZE\n",
    "  frames = [sample[idx * FRAME_SIZE : FRAME_SIZE + idx * FRAME_SIZE] for idx in range(number_of_frames)]\n",
    "\n",
    "  frames = np.array(frames)\n",
    "\n",
    "  fft = np.array([convert_imag_to_parts(np.fft.fft(frame)) for frame in frames])\n",
    "\n",
    "  output_frames = model.predict([frames, fft, np.ones((frames.shape[0], 1)) * fs])\n",
    "\n",
    "  final_frame = None\n",
    "  \n",
    "  for frame in output_frames:\n",
    "    frame = np.reshape(frame, (FRAME_SIZE,))\n",
    "\n",
    "    if final_frame is None:\n",
    "      final_frame = frame\n",
    "    else:\n",
    "      final_frame = np.concatenate([final_frame, frame], axis=0)\n",
    "\n",
    "  return final_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_orig, sample_freq = sf.read(\"../audio/xdousa00.wav\")\n",
    "\n",
    "samples_normal = normalization(samples_orig) + 1\n",
    "samples_normal = normalization(samples_normal)\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.title(\"Normalizovaný vstupní signál\")\n",
    "plt.plot(np.arange(samples_normal.size) / sample_freq, samples_normal)\n",
    "plt.gca().set_xlabel('$t[s]$')\n",
    "plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "plt.show()\n",
    "\n",
    "cleared_signal = denoise(samples_normal, sample_freq)\n",
    "norm_cleared_signal = normalization(cleared_signal)\n",
    "\n",
    "print(norm_cleared_signal.shape)\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.title(\"Vyčištěný signál\")\n",
    "plt.plot(np.arange(norm_cleared_signal.size) / sample_freq, norm_cleared_signal)\n",
    "plt.gca().set_xlabel('$t[s]$')\n",
    "plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "plt.show()\n",
    "\n",
    "wavfile.write(\"clean_test.wav\", sample_freq, norm_cleared_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Are you sure you want to delete datasets?\\n\") == \"y\":\n",
    "  shutil.rmtree(TRAIN_DATASET_PATH)\n",
    "  shutil.rmtree(TEST_DATASET_PATH)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b65fa5fc1f58c1d32dad6f9c9505eeadcca0e46e66251bfc0668e571e415692e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
