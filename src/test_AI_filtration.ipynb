{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, optimizers, Model, callbacks, initializers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from pydub import AudioSegment\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "import soundfile as sf\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy import signal\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "\n",
    "TRAIN_DATA_PATH = r\"..\\\\data_train\"\n",
    "TEST_DATA_PATH = r\"..\\\\data_test\"\n",
    "TRAIN_DATASET_PATH = r\"..\\\\train_dataset\"\n",
    "TEST_DATASET_PATH = r\"..\\\\test_dataset\"\n",
    "\n",
    "MAX_SAMPLE_FREQUENCY = 256_000\n",
    "\n",
    "FRAME_SIZE = 4096\n",
    "FRAME_OVERLAP = 2048\n",
    "\n",
    "MAXIMUM_NUMBER_OF_NOISE_CHANNELS = 8\n",
    "NOISE_APLITUDE_MAX = 4\n",
    "NOISE_FREQ_MAX = 48_000\n",
    "\n",
    "EPOCHS = 10\n",
    "STEPES_PER_EPOCH = 5000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "frame_shift = FRAME_SIZE - FRAME_OVERLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(files, dirs=[], extensions=[]):\n",
    "    new_dirs = []\n",
    "    for d in dirs:\n",
    "        try:\n",
    "            new_dirs += [ os.path.join(d, f) for f in os.listdir(d) ]\n",
    "        except OSError:\n",
    "            if os.path.splitext(d)[1] in extensions:\n",
    "                files.append(d)\n",
    "\n",
    "    if new_dirs:\n",
    "        find_files(files, new_dirs, extensions)\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "file_paths = []\n",
    "find_files(file_paths, dirs=[TRAIN_DATA_PATH, TEST_DATA_PATH], extensions=[\".mp3\"])\n",
    "print(len(file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all mp3 files to wav\n",
    "def convert_all_files():\n",
    "    for file_path in file_paths:\n",
    "        if file_path[-4:] == \".mp3\":\n",
    "            cleaned_name = file_path[:-4]\n",
    "            new_name = cleaned_name + \".wav\"\n",
    "\n",
    "            sound = AudioSegment.from_file(file_path, format=\"mp3\")\n",
    "            sound.export(new_name, \"wav\")\n",
    "\n",
    "            os.remove(file_path)\n",
    "\n",
    "convert_all_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(samples:np.ndarray):\n",
    "    smpl = samples.copy()\n",
    "    max_abs_val = max(abs(smpl))\n",
    "\n",
    "    # Normalization\n",
    "    smpl /= max_abs_val\n",
    "    return smpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_data(data, fs):\n",
    "  sdata = data.copy()\n",
    "\n",
    "  length_in_secs = sdata.size / fs\n",
    "  time = np.linspace(0, length_in_secs, sdata.size, endpoint=False)\n",
    "\n",
    "  for _ in range(random.randint(1, MAXIMUM_NUMBER_OF_NOISE_CHANNELS)):\n",
    "    sdata += (random.random() * NOISE_APLITUDE_MAX) * np.cos(2 * np.pi * (random.random() * NOISE_FREQ_MAX) * time)\n",
    "  \n",
    "  return sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frames(data:np.ndarray):\n",
    "  number_of_frames = data.size // frame_shift\n",
    "  frames = [data[idx * frame_shift : FRAME_SIZE + idx * frame_shift] for idx in range(number_of_frames)]\n",
    "\n",
    "  # Pad last frame with zeroes\n",
    "  frames[len(frames) - 1] = np.pad(frames[len(frames) - 1], (0, FRAME_SIZE - frames[len(frames) - 1].shape[0]), \"constant\")\n",
    "  return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawFFT(signal, frequency, idx=None):\n",
    "  fft = np.fft.fft(signal)\n",
    "\n",
    "  data_length = len(signal)\n",
    "  freq = np.arange(data_length)/(data_length/frequency)\n",
    "\n",
    "  half_length = data_length//2\n",
    "\n",
    "  freq_one_side = freq[:half_length]\n",
    "  fft = fft[:half_length]/half_length\n",
    "\n",
    "  plt.figure(figsize=(18,8))\n",
    "  plt.title(f\"FFT {idx}\" if idx is not None else \"Generic FFT\")\n",
    "  plt.plot(freq_one_side, abs(fft))\n",
    "  plt.xlabel('f[Hz]')\n",
    "  plt.ylabel('Amplituda[-]')\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_file(smpls, fs):\n",
    "  # Split and normalize\n",
    "  try:\n",
    "    if smpls.shape[1] > 0:\n",
    "      normalized_samples = []\n",
    "\n",
    "      for i in range(smpls.shape[1]):\n",
    "        normalized_samples.extend(normalization(smpls[:, i]))\n",
    "\n",
    "      normalized_samples = np.array(normalized_samples)\n",
    "    else:\n",
    "      normalized_samples = normalization(smpls)\n",
    "  except Exception as e:\n",
    "    print(f\"Exception on spliting data\\n{e}\")\n",
    "    normalized_samples = normalization(smpls)\n",
    "\n",
    "  # Create noised data and normalize them\n",
    "  noised_samples = noise_data(normalized_samples, fs)\n",
    "  noised_samples = normalization(noised_samples)\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(normalized_samples.size) / fs, normalized_samples)\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(noised_samples.size) / fs, noised_samples)\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  # Power density\n",
    "  # f1, Pxx1 = signal.periodogram(normalized_samples, fs, scaling=\"density\")\n",
    "  # print(f1.shape)\n",
    "  # print(Pxx1.shape)\n",
    "\n",
    "  # plt.semilogy(f1, Pxx1)\n",
    "  # plt.title(\"original\")\n",
    "  # plt.xlabel('f[Hz]')\n",
    "  # plt.ylabel('PSD [V**2/Hz]')\n",
    "  # plt.show()\n",
    "\n",
    "  # f2, Pxx2 = signal.periodogram(noised_samples, fs, scaling=\"density\")\n",
    "  # print(f2.shape)\n",
    "  # print(Pxx2.shape)\n",
    "\n",
    "  # plt.semilogy(f2, Pxx2)\n",
    "  # plt.title(\"noised\")\n",
    "  # plt.xlabel('f[Hz]')\n",
    "  # plt.ylabel('PSD [V**2/Hz]')\n",
    "  # plt.show()\n",
    "\n",
    "  # FFT\n",
    "  # normal_fft = np.fft.fft(normalized_samples)\n",
    "  # noise_fft = np.fft.fft(noised_samples)\n",
    "\n",
    "  # print(normal_fft.shape)\n",
    "  # print(noise_fft.shape)\n",
    "\n",
    "  # data_length = len(normal_fft)\n",
    "  # freq = np.arange(data_length)/(data_length/fs)\n",
    "\n",
    "  # half_length = data_length//2\n",
    "\n",
    "  # freq_one_side = freq[:half_length]\n",
    "  # normal_fft = normal_fft[:half_length]/half_length\n",
    "  # noise_fft = noise_fft[:half_length]/half_length\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.title(f\"Normal\")\n",
    "  # plt.plot(freq_one_side, abs(normal_fft))\n",
    "  # plt.xlabel('f[Hz]')\n",
    "  # plt.ylabel('Amplituda[-]')\n",
    "  # plt.tight_layout()\n",
    "  # plt.show()\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.title(f\"Noise\")\n",
    "  # plt.plot(freq_one_side, abs(noise_fft))\n",
    "  # plt.xlabel('f[Hz]')\n",
    "  # plt.ylabel('Amplituda[-]')\n",
    "  # plt.tight_layout()\n",
    "  # plt.show()\n",
    "\n",
    "  # Set signals to be between 0 and 1\n",
    "  normalized_samples += 1\n",
    "  noised_samples += 1\n",
    "  normalized_samples = normalization(normalized_samples)\n",
    "  noised_samples = normalization(noised_samples)\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(normalized_samples.size) / fs, normalized_samples)\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(noised_samples.size) / fs, noised_samples)\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  # Create frames\n",
    "  normalized_samples = create_frames(normalized_samples)\n",
    "  noised_samples = create_frames(noised_samples)\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(normalized_samples[50].size) / fs, normalized_samples[50])\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  # plt.figure(figsize=(18,8))\n",
    "  # plt.plot(np.arange(noised_samples[50].size) / fs, noised_samples[50])\n",
    "  # plt.gca().set_xlabel('$t[s]$')\n",
    "  # plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "  # plt.show()\n",
    "\n",
    "  return normalized_samples, noised_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 2/23 [00:00<00:01, 19.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Y2Mate.is - Can 3 Gamers Beat 1 Trackmania Pro Driving Backwards-DqmSovzS8HY-48k-1639525438813\n",
      "Skipping Y2Mate.is - CHUNGUS 2 - A very powerful 1Hz Minecraft CPU-FDiapbD0Xfg-128k-1639551674784\n",
      "Skipping Y2Mate.is - Hallelujah - Leonard Cohen [INSANE Piano Cover]-dp6wHzfAH7k-48k-1638725278647\n",
      "Skipping Y2Mate.is - How to solder components using hot air without them blowing away-gl9NuCiDR7s-48k-1639538474391\n",
      "Skipping Y2Mate.is - How to Wire Up Ethernet Plugs the EASY WAY! (Cat5e  Cat6 RJ45 Pass Through Connectors)-NWhoJp8UQpo-48k-1639516438346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 8/23 [00:00<00:00, 20.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Y2Mate.is - I Challenged 20,000 People to a game of Geoguessr.-8xP9dzxOOGw-128k-1639551233412\n",
      "Skipping Y2Mate.is - I played a Trackmania tournament where backwards is faster...-U5qP2cB0E7k-48k-1639520188975\n",
      "Skipping Y2Mate.is - International Drug Smuggler On How He Beat Airport Security  Minutes With  UNILAD  @LADbible TV-QEPFWgLwrFw-64k-1639550374941\n",
      "Skipping Y2Mate.is - Magnetic dough and fluid...-wRLnwLsdK6A-48k-1639538350175\n",
      "Skipping Y2Mate.is - Making a bolt with ZIGZAG threads - SO STRANGE! - Lost PLA metal casting-cvi8A2XCK94-160k-1639551618509\n",
      "Skipping Y2Mate.is - My Chemical Romance - Famous Last Words [Official Music Video] [HD]-8bbTtPL1jRs-48k-1639519742528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 13/23 [00:00<00:00, 12.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Y2Mate.is - Panama Canal Toll Fee  How Much Do Ships Pay  Chief MAKOi Seaman Vlog-_FxBBE8DfJI-64k-1639551563045\n",
      "Skipping Y2Mate.is - Proč Je Tento Havajský Ostrov Pro Veřejnost Uzavřen Více Než 100 Let Zakázaný Ostrov - Niihau-nxzFwbx0i9Q-160k-1639551446148\n",
      "Skipping Y2Mate.is - Sommelier Shops For Holiday Wines (14 Different Occasions)  World Of Wine  Bon Appétit-eygxbraz9v8-128k-1639551339499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 17/23 [00:01<00:00, 14.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Y2Mate.is - Stupid C++ Tricks Most Dangerous C Functions (E02)-B9DouAlkZlc-48k-1639516194897\n",
      "Skipping Y2Mate.is - The Big Misconception About Electricity-bHIhgxav9LY-160k-1638729545425\n",
      "Skipping Y2Mate.is - The Biggest Accidents And Injuries On MythBusters-CokW5No-vIU-128k-1639550425817\n",
      "Skipping Y2Mate.is - The Elder Scrolls Online After 1000 Hours-ECoXQjFFr7s-160k-1639551763030\n",
      "Skipping Y2Mate.is - These TikTok ‘’Musicians’’ Need to STOP-CIhzQIFmZTI-48k-1639529539528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [00:01<00:00, 15.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Y2Mate.is - Trick Your Sense Of Taste In 5 Different Ways-6pTv-2XPBl4-48k-1639529673526\n",
      "Skipping Y2Mate.is - Watch Escapement Desk Toy-RcnWL6_6Xu4-96k-1639551819659\n",
      "Skipping Y2Mate.is - When you accidentally write theme songs that already exist-K-sGekaaGJA-48k-1639515859068\n",
      "Skipping Y2Mate.is - Why Ernest Wright Scissors Are So Expensive  So Expensive-bK4AWtTV3h4-48k-1639529628215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_paths = []\n",
    "find_files(file_paths, dirs=[TRAIN_DATA_PATH], extensions=[\".wav\"])\n",
    "print(len(file_paths))\n",
    "\n",
    "if not os.path.exists(TRAIN_DATASET_PATH):\n",
    "  os.mkdir(TRAIN_DATASET_PATH)\n",
    "\n",
    "for file_path in tqdm(file_paths):\n",
    "  cleaned_name = file_path[:-4].split(\"\\\\\")[-1]\n",
    "\n",
    "  if not os.path.exists(f\"{TRAIN_DATASET_PATH}/{cleaned_name}\"):\n",
    "    os.mkdir(f\"{TRAIN_DATASET_PATH}/{cleaned_name}\")\n",
    "\n",
    "    smpls, f = sf.read(file_path)\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Processing {cleaned_name}\")\n",
    "      \n",
    "    random.seed()\n",
    "    norm_s, nois_s = process_one_file(smpls, f)\n",
    "\n",
    "    offset = len(os.listdir(f\"{TRAIN_DATASET_PATH}/{cleaned_name}\"))\n",
    "\n",
    "    for idx, (a, b) in enumerate(zip(norm_s, nois_s)):\n",
    "      if not os.path.exists(f\"{TRAIN_DATASET_PATH}/{cleaned_name}/{idx + offset}_{file_path}\"):\n",
    "        np.save(f\"{TRAIN_DATASET_PATH}/{cleaned_name}/{idx + offset}_{f}\", np.array([a, b, np.fft.fft(b)]))\n",
    "  else:\n",
    "    print(f\"Skipping {cleaned_name}\")\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:00<00:00, 14.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Y2Mate.is - 15 Biggest And Most Expensive Mining FINDS-GAwIKLthp-U-48k-1639522068716\n",
      "Skipping Y2Mate.is - Chernobyl (2019) - It's not 3 Roentgen it is 15000 scene.-yPRMx2k1NM8-128k-1639552069014\n",
      "Skipping Y2Mate.is - CHUNGUS 2 - A very powerful 1Hz Minecraft CPU-FDiapbD0Xfg-160k-1639551674263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:00<00:00, 17.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Y2Mate.is - I Challenged 20,000 People to a game of Geoguessr.-8xP9dzxOOGw-64k-1639551249719\n",
      "Skipping Y2Mate.is - Making a bolt with ZIGZAG threads - SO STRANGE! - Lost PLA metal casting-cvi8A2XCK94-64k-1639551619488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:00<00:00, 18.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Y2Mate.is - Proč Je Tento Havajský Ostrov Pro Veřejnost Uzavřen Více Než 100 Let Zakázaný Ostrov - Niihau-nxzFwbx0i9Q-96k-1639551444666\n",
      "Skipping Y2Mate.is - Sommelier Shops For Holiday Wines (14 Different Occasions)  World Of Wine  Bon Appétit-eygxbraz9v8-160k-1639551332303\n",
      "Skipping Y2Mate.is - The Big Misconception About Electricity-bHIhgxav9LY-128k-1639551389489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 18.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Y2Mate.is - Watch Escapement Desk Toy-RcnWL6_6Xu4-160k-1639551826187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "file_paths = []\n",
    "find_files(file_paths, dirs=[TEST_DATA_PATH], extensions=[\".wav\"])\n",
    "print(len(file_paths))\n",
    "\n",
    "if not os.path.exists(TEST_DATASET_PATH):\n",
    "  os.mkdir(TEST_DATASET_PATH)\n",
    "\n",
    "for file_path in tqdm(file_paths):\n",
    "  cleaned_name = file_path[:-4].split(\"\\\\\")[-1]\n",
    "\n",
    "  if not os.path.exists(f\"{TEST_DATASET_PATH}/{cleaned_name}\"):\n",
    "    os.mkdir(f\"{TEST_DATASET_PATH}/{cleaned_name}\")\n",
    "\n",
    "    smpls, f = sf.read(file_path)\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Processing {cleaned_name}\")\n",
    "      \n",
    "    random.seed()\n",
    "    norm_s, nois_s = process_one_file(smpls, f)\n",
    "\n",
    "    offset = len(os.listdir(f\"{TEST_DATASET_PATH}/{cleaned_name}\"))\n",
    "\n",
    "    for idx, (a, b) in enumerate(zip(norm_s, nois_s)):\n",
    "      if not os.path.exists(f\"{TEST_DATASET_PATH}/{cleaned_name}/{idx + offset}_{file_path}\"):\n",
    "        np.save(f\"{TEST_DATASET_PATH}/{cleaned_name}/{idx + offset}_{f}\", np.array([a, b, np.fft.fft(b)]))\n",
    "  else:\n",
    "    print(f\"Skipping {cleaned_name}\")\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_imag_to_parts(array:np.ndarray):\n",
    "    tmp = np.empty((array.shape[0], 2))\n",
    "\n",
    "    for idx, a in enumerate(array):\n",
    "        tmp[idx][0] = a.real\n",
    "        tmp[idx][1] = a.imag\n",
    "\n",
    "    return np.nan_to_num(tmp)\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence, threading.Thread):\n",
    "    def __init__(self, path, dim, batch_size=32, shuffle=True):\n",
    "        super(DataGenerator, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.files = []\n",
    "        find_files(self.files, dirs=[path], extensions=[\".npy\"])\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.queue = []\n",
    "\n",
    "        self.length = int(np.floor(len(self.files) / self.batch_size))\n",
    "        self.index = 0\n",
    "        self.daemon = True\n",
    "        self.__terminate = False;\n",
    "        self.shuffle_data()\n",
    "        self.start()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.__terminate = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def run(self) -> None:\n",
    "        while True:\n",
    "            if self.__terminate: break\n",
    "\n",
    "            if len(self.queue) < 20:\n",
    "                if (self.index + 1) >= self.length:\n",
    "                    self.shuffle_data()\n",
    "                    self.index = 0\n",
    "\n",
    "                files = self.files[self.index*self.batch_size:(self.index+1)*self.batch_size]\n",
    "                self.index += 1\n",
    "\n",
    "                # Generate data\n",
    "                self.queue.append(self.__data_generation(files))\n",
    "            else: time.sleep(0.1)\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        while len(self.queue) == 0: time.sleep(0.01)\n",
    "        return self.queue.pop()\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.files)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def __data_generation(self, files):\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        fft = np.empty((self.batch_size, self.dim[0], 2))\n",
    "        f = np.empty((self.batch_size), dtype=float)\n",
    "        y = np.empty((self.batch_size, *self.dim))\n",
    "\n",
    "        # Generate data\n",
    "        for idx, file in enumerate(files):\n",
    "            freq = float(file[:-4].split(\"_\")[-1])\n",
    "            loaded_data = np.load(file)\n",
    "\n",
    "            X[idx,] = loaded_data[1].real\n",
    "            fft[idx,] = convert_imag_to_parts(loaded_data[2])\n",
    "            f[idx] = freq / MAX_SAMPLE_FREQUENCY\n",
    "            y[idx,] = loaded_data[0].real\n",
    "\n",
    "        return [X, fft, f], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm_value = 2.0\n",
    "\n",
    "def create_model():\n",
    "  inp1 = layers.Input(shape=(FRAME_SIZE,1), name=\"Frame Input\")\n",
    "  inp2 = layers.Input(shape=(FRAME_SIZE,2), name=\"FFT Input\")\n",
    "  inp3 = layers.Input(shape=(1,), name=\"Frequency Input\")\n",
    "\n",
    "  y = layers.Dense(FRAME_SIZE)(inp3)\n",
    "  y = layers.LeakyReLU(0.2)(y)\n",
    "  y = layers.Dropout(0.1)(y)\n",
    "  y = layers.Reshape((FRAME_SIZE, 1))(y)\n",
    "\n",
    "  x1 = layers.concatenate([inp1, inp2, y])\n",
    "\n",
    "  # Encoder\n",
    "  x = layers.Conv1D(64, kernel_size=3, strides=2, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x1)\n",
    "  x2 = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.Conv1D(32, kernel_size=3, strides=2, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x2)\n",
    "  x3 = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.Conv1D(32, kernel_size=3, strides=2, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x3)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "  \n",
    "  # Encoded value\n",
    "\n",
    "  # Edecoder\n",
    "  x = layers.Conv1D(16, kernel_size=3, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n",
    "  x = layers.UpSampling1D(2)(x)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.concatenate([x, x3])\n",
    "\n",
    "  x = layers.Conv1D(32, kernel_size=3, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n",
    "  x = layers.UpSampling1D(2)(x)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  x = layers.concatenate([x, x2])\n",
    "\n",
    "  x = layers.Conv1D(64, kernel_size=3, padding=\"same\", kernel_constraint=max_norm(max_norm_value), kernel_initializer=initializers.RandomNormal(stddev=0.02))(x)\n",
    "  x = layers.UpSampling1D(2)(x)\n",
    "  x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "  # Output\n",
    "  x = layers.concatenate([x, x1])\n",
    "  x = layers.Conv1D(1, kernel_size=1, kernel_constraint=max_norm(max_norm_value), activation='sigmoid', padding='same')(x)\n",
    "  return Model([inp1, inp2, inp3], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11067\n",
      "4413\n"
     ]
    }
   ],
   "source": [
    "train_data_generator = DataGenerator(TRAIN_DATASET_PATH, (FRAME_SIZE,), BATCH_SIZE)\n",
    "test_data_generator = DataGenerator(TEST_DATASET_PATH, (FRAME_SIZE,), BATCH_SIZE)\n",
    "\n",
    "print(len(train_data_generator))\n",
    "print(len(test_data_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[0.32195255 0.35937659 0.51424035 ... 0.50665731 0.15910732 0.26415681]\n",
      " [0.42604742 0.43227304 0.15106385 ... 0.28526474 0.30343954 0.3018026 ]\n",
      " [0.41524736 0.48580823 0.38627286 ... 0.40860546 0.90587386 0.56238881]\n",
      " ...\n",
      " [0.62683545 0.142085   0.48662896 ... 0.51710423 0.20210837 0.53995452]\n",
      " [0.33090017 0.34050969 0.54189299 ... 0.50271194 0.51316338 0.77909505]\n",
      " [0.3355855  0.47607532 0.468097   ... 0.41958472 0.42521867 0.69081864]]\n",
      "[[[ 2.05452839e+03  0.00000000e+00]\n",
      "  [-1.49433782e-01 -3.93938960e-04]\n",
      "  [-1.48367096e-01  1.11319907e-02]\n",
      "  ...\n",
      "  [-1.13780679e-01 -1.77125877e-02]\n",
      "  [-1.48367096e-01 -1.11319907e-02]\n",
      "  [-1.49433782e-01  3.93938960e-04]]\n",
      "\n",
      " [[ 2.05454044e+03  0.00000000e+00]\n",
      "  [-1.55331291e-01 -5.97437158e-04]\n",
      "  [-1.68909295e-01 -3.69641692e-02]\n",
      "  ...\n",
      "  [-1.34244728e-01  5.05900103e-02]\n",
      "  [-1.68909295e-01  3.69641692e-02]\n",
      "  [-1.55331291e-01  5.97437158e-04]]\n",
      "\n",
      " [[ 2.04830441e+03  0.00000000e+00]\n",
      "  [ 3.73630774e-01 -3.00463409e-02]\n",
      "  [ 2.81158485e-01 -6.05177904e-02]\n",
      "  ...\n",
      "  [ 1.27007831e-01 -9.00974273e-02]\n",
      "  [ 2.81158485e-01  6.05177904e-02]\n",
      "  [ 3.73630774e-01  3.00463409e-02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.04770107e+03  0.00000000e+00]\n",
      "  [-1.13388112e-01 -3.79765820e-02]\n",
      "  [-1.27181827e-01 -2.18934915e-01]\n",
      "  ...\n",
      "  [-3.07523640e-01  3.16182196e-01]\n",
      "  [-1.27181827e-01  2.18934915e-01]\n",
      "  [-1.13388112e-01  3.79765820e-02]]\n",
      "\n",
      " [[ 2.04785743e+03  0.00000000e+00]\n",
      "  [-2.65897122e-01 -3.77110355e-02]\n",
      "  [-2.54251043e-01  2.56726925e-02]\n",
      "  ...\n",
      "  [-2.34855623e-01  3.13480965e-02]\n",
      "  [-2.54251043e-01 -2.56726925e-02]\n",
      "  [-2.65897122e-01  3.77110355e-02]]\n",
      "\n",
      " [[ 2.05492902e+03  0.00000000e+00]\n",
      "  [ 2.33984419e-01 -1.18814320e-02]\n",
      "  [ 2.34362743e-01 -1.75769340e-02]\n",
      "  ...\n",
      "  [ 2.25830600e-01  2.43737715e-02]\n",
      "  [ 2.34362743e-01  1.75769340e-02]\n",
      "  [ 2.33984419e-01  1.18814320e-02]]]\n",
      "[0.17226563 0.17226563 0.1875     0.1875     0.17226563 0.1875\n",
      " 0.1875     0.17226563 0.17226563 0.1875     0.17226563 0.1875\n",
      " 0.1875     0.1875     0.17226563 0.17226563 0.1875     0.1875\n",
      " 0.1875     0.17226563 0.17226563 0.1875     0.17226563 0.1875\n",
      " 0.17226563 0.1875     0.1875     0.17226563 0.17226563 0.17226563\n",
      " 0.17226563 0.17226563 0.1875     0.17226563 0.17226563 0.17226563\n",
      " 0.17226563 0.17226563 0.17226563 0.1875     0.17226563 0.1875\n",
      " 0.17226563 0.17226563 0.17226563 0.17226563 0.17226563 0.1875\n",
      " 0.1875     0.17226563 0.1875     0.17226563 0.17226563 0.17226563\n",
      " 0.1875     0.17226563 0.17226563 0.17226563 0.1875     0.1875\n",
      " 0.1875     0.1875     0.1875     0.17226563]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.float64'>\n",
      "[0.32195255 0.35937659 0.51424035 ... 0.50665731 0.15910732 0.26415681]\n",
      "[[ 2.05452839e+03  0.00000000e+00]\n",
      " [-1.49433782e-01 -3.93938960e-04]\n",
      " [-1.48367096e-01  1.11319907e-02]\n",
      " ...\n",
      " [-1.13780679e-01 -1.77125877e-02]\n",
      " [-1.48367096e-01 -1.11319907e-02]\n",
      " [-1.49433782e-01  3.93938960e-04]]\n",
      "0.172265625\n",
      "[[0.49343373 0.49178715 0.49078313 ... 0.45801205 0.45072289 0.44168675]\n",
      " [0.48198795 0.4814257  0.47688755 ... 0.48355422 0.49150602 0.49801205]\n",
      " [0.45090241 0.44302827 0.43804504 ... 0.49463345 0.49552787 0.49741255]\n",
      " ...\n",
      " [0.39638277 0.39292714 0.38838841 ... 0.46124884 0.45973593 0.45796513]\n",
      " [0.51923015 0.51806421 0.52763137 ... 0.5671618  0.57091519 0.57961987]\n",
      " [0.51664659 0.52429719 0.52961847 ... 0.4836747  0.4873494  0.49222892]]\n"
     ]
    }
   ],
   "source": [
    "test_data = train_data_generator[0]\n",
    "\n",
    "print(type(test_data[0]))\n",
    "print(type(test_data[1]))\n",
    "\n",
    "print(type(test_data[0][0]))\n",
    "print(type(test_data[0][1]))\n",
    "print(type(test_data[0][2]))\n",
    "\n",
    "print(test_data[0][0])\n",
    "print(test_data[0][1])\n",
    "print(test_data[0][2])\n",
    "\n",
    "print(type(test_data[0][0][0]))\n",
    "print(type(test_data[0][1][0]))\n",
    "print(type(test_data[0][2][0]))\n",
    "\n",
    "print(test_data[0][0][0])\n",
    "print(test_data[0][1][0])\n",
    "print(test_data[0][2][0])\n",
    "\n",
    "print(test_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noiseToSignalLoss(y_true, y_pred):\n",
    "    losses = tf.math.divide(tf.math.reduce_sum(tf.math.pow(tf.math.abs(tf.math.subtract(y_true,y_pred)),2)),\n",
    "                            tf.math.reduce_sum(tf.math.pow(tf.math.abs(y_true),2)))\n",
    "    return tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSNR(y_true, y_pred):\n",
    "  return -10.0 * K.log(K.mean(K.square(y_pred - y_true))) / K.log(10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Frequency Input (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 4096)         8192        ['Frequency Input[0][0]']        \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 4096)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096)         0           ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " Frame Input (InputLayer)       [(None, 4096, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " FFT Input (InputLayer)         [(None, 4096, 2)]    0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 4096, 1)      0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4096, 4)      0           ['Frame Input[0][0]',            \n",
      "                                                                  'FFT Input[0][0]',              \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 2048, 64)     832         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 2048, 64)     0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 1024, 32)     6176        ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 1024, 32)     0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 512, 32)      3104        ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 512, 32)      0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 512, 16)      1552        ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling1d (UpSampling1D)   (None, 1024, 16)     0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 1024, 16)     0           ['up_sampling1d[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 1024, 48)     0           ['leaky_re_lu_4[0][0]',          \n",
      "                                                                  'leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 1024, 32)     4640        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling1d_1 (UpSampling1D)  (None, 2048, 32)    0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 2048, 32)     0           ['up_sampling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 2048, 96)     0           ['leaky_re_lu_5[0][0]',          \n",
      "                                                                  'leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 2048, 64)     18496       ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " up_sampling1d_2 (UpSampling1D)  (None, 4096, 64)    0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 4096, 64)     0           ['up_sampling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 4096, 68)     0           ['leaky_re_lu_6[0][0]',          \n",
      "                                                                  'concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 4096, 1)      69          ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 43,061\n",
      "Trainable params: 43,061\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[(None, 4096, 1), (None, 4096, 2), (None, 1)]\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()\n",
    "# tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "print(model.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.compile(optimizer=optimizers.Adam(0.0001, 0.5), loss=noiseToSignalLoss, metrics=[PSNR])\n",
    "# model.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5000/5000 [==============================] - 1464s 285ms/step - loss: 0.0131 - PSNR: 24.9343 - val_loss: 0.0087 - val_PSNR: 26.7263 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "3759/5000 [=====================>........] - ETA: 4:38 - loss: 0.0090 - PSNR: 26.4801"
     ]
    }
   ],
   "source": [
    "number_of_batches = len(train_data_generator) // STEPES_PER_EPOCH\n",
    "\n",
    "if os.path.exists(\"weights.h5\"):\n",
    "  model.load_weights(\"weights.h5\")\n",
    "\n",
    "try:\n",
    "  model.fit(train_data_generator, steps_per_epoch=STEPES_PER_EPOCH, epochs=EPOCHS * number_of_batches, callbacks=[callbacks.ReduceLROnPlateau(monitor='val_loss',patience=2,factor=0.2,verbose=1), callbacks.TensorBoard()],\n",
    "            validation_data=test_data_generator, validation_steps=(STEPES_PER_EPOCH // 4) if STEPES_PER_EPOCH is not None else None)\n",
    "except KeyboardInterrupt:\n",
    "  model.save_weights(\"weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\", include_optimizer=False, save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(sample, fs):\n",
    "  number_of_frames = sample.size // FRAME_SIZE\n",
    "  frames = np.array([sample[idx * FRAME_SIZE : FRAME_SIZE + idx * FRAME_SIZE] for idx in range(number_of_frames)])\n",
    "  fft = np.array([convert_imag_to_parts(np.fft.fft(frame)) for frame in frames])\n",
    "\n",
    "  output_frames = model.predict([frames, fft, np.ones((frames.shape[0], 1)) * fs])\n",
    "\n",
    "  final_frame = None\n",
    "  for out_frame in output_frames:\n",
    "    if final_frame is None:\n",
    "      final_frame = out_frame\n",
    "    else:\n",
    "      np.concatenate((final_frame, out_frame), axis=0)\n",
    "  return final_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_orig, sample_freq = sf.read(\"../audio/xdousa00.wav\")\n",
    "\n",
    "samples_normal = normalization(samples_orig) + 1\n",
    "samples_normal = normalization(samples_normal)\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.title(\"Normalizovaný vstupní signál\")\n",
    "plt.plot(np.arange(samples_normal.size) / sample_freq, samples_normal)\n",
    "plt.gca().set_xlabel('$t[s]$')\n",
    "plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "plt.show()\n",
    "\n",
    "cleared_signal = denoise(samples_normal, sample_freq)\n",
    "norm_cleared_signal = normalization(cleared_signal)\n",
    "\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.title(\"Vyčištěný signál\")\n",
    "plt.plot(np.arange(norm_cleared_signal.size) / sample_freq, norm_cleared_signal)\n",
    "plt.gca().set_xlabel('$t[s]$')\n",
    "plt.gca().set_ylabel('$Amplituda[-]$')\n",
    "plt.show()\n",
    "\n",
    "wavfile.write(\"clean_test.wav\", sample_freq, norm_cleared_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Are you sure you want to delete datasets?\\n\") == \"y\":\n",
    "  shutil.rmtree(TRAIN_DATASET_PATH)\n",
    "  shutil.rmtree(TEST_DATASET_PATH)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b65fa5fc1f58c1d32dad6f9c9505eeadcca0e46e66251bfc0668e571e415692e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
